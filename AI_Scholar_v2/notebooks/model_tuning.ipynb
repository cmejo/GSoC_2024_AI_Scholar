{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6AHgTUefW7X"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries if you haven't already"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "from datasets import load_dataset, Dataset"
      ],
      "metadata": {
        "id": "gD4xT26NfcIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Llama3 model and tokenizer"
      ],
      "metadata": {
        "id": "nvXkTXYgfhJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"Meta-Llama-3.1-8B\"  # Replace with your model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")"
      ],
      "metadata": {
        "id": "dLzYeYLDfg80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and preprocess your dataset"
      ],
      "metadata": {
        "id": "2MM0kwYYfgyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = pdf_to_text('../data/pdfs')  # Reuse function from data exploration"
      ],
      "metadata": {
        "id": "av2zSNF_fgoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a dataset for fine-tuning"
      ],
      "metadata": {
        "id": "OcpJxGNifgds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "dataset = Dataset.from_pandas(pd.DataFrame(texts))\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "JeQPza8XfgUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning configuration"
      ],
      "metadata": {
        "id": "0-xiBne9fgIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")"
      ],
      "metadata": {
        "id": "JS7lTcG8f5a6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tune the model\n"
      ],
      "metadata": {
        "id": "bsGfXBzHf7G8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    eval_dataset=tokenized_dataset,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "iR85VeA3f84T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the fine-tuned model\n"
      ],
      "metadata": {
        "id": "NNlT-u7Cf-4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained('./fine_tuned_model')\n",
        "tokenizer.save_pretrained('./fine_tuned_model')\n"
      ],
      "metadata": {
        "id": "TyZ47eJygAgy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
