{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# commands run:\n",
    "\n",
    "## for langchain & langflow environment\n",
    "1. pip install langchain\n",
    "2. pip3 install torch torchvision\n",
    "3. pip install langchain langsmith langchain-community langgraph langchain-cli langchainhub langchain-openai langchain-chroma bs4\n",
    "\n",
    "## langflow start\n",
    "\n",
    "python3 -m langflow run\n",
    "\n",
    "\n",
    "## remote login\n",
    "\n",
    "ssh -C -p 222 -L 7860:127.0.0.1:7860 cmejo@rstudio-tr.braverock.com # langflow http://localhost:7860/ \n",
    "ssh -C -p 222 -L 11434:127.0.0.1:11434 cmejo@rstudio-tr.braverock.com # for ollama http://localhost:11434/ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and preprocess custom dataset\n",
    "\n",
    "import json\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "\n",
    "# Load a pre-trained LLaMA model and tokenizer\n",
    "model_name = \"meta-llama/llama3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Load custom dataset\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "dataset_path = \"path/to/your/dataset.json\"\n",
    "dataset = load_dataset(dataset_path)\n",
    "\n",
    "# Generate embeddings for the dataset\n",
    "def generate_embeddings(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "embeddings_list = [generate_embeddings(doc[\"text\"], tokenizer, model) for doc in dataset]\n",
    "embeddings_array = np.array(embeddings_list)\n",
    "\n",
    "# Create a FAISS index\n",
    "dimension = embeddings_array.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings_array)\n",
    "\n",
    "# Save the index\n",
    "faiss.write_index(index, \"faiss_index.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# langfllow configuration \n",
    "\n",
    "from langchain.chains import SimpleChain\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import Prompt\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Load the FAISS index\n",
    "index = faiss.read_index(\"faiss_index.bin\")\n",
    "\n",
    "# Define a prompt template\n",
    "prompt_template = \"\"\"\n",
    "You are an AI assistant specialized in answering questions based on the provided dataset. Given the following document text, provide a brief and accurate answer to the question:\n",
    "Document: {document}\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Define the function to find the most relevant document\n",
    "def find_relevant_document(question, index, dataset, embeddings):\n",
    "    query_embedding = embeddings.generate_embeddings(question).reshape(1, -1)\n",
    "    D, I = index.search(query_embedding, k=1)\n",
    "    document = dataset[I[0][0]][\"text\"]\n",
    "    return document\n",
    "\n",
    "# Define the chatbot function\n",
    "def rag_chatbot(question, index, dataset, embeddings):\n",
    "    document = find_relevant_document(question, index, dataset, embeddings)\n",
    "    prompt = Prompt(prompt_template.format(document=document, question=question))\n",
    "    answer = prompt.run()\n",
    "    return answer\n",
    "\n",
    "# Example usage\n",
    "question = \"What is quantum entanglement?\"\n",
    "answer = rag_chatbot(question, index, dataset, HuggingFaceEmbeddings(model, tokenizer))\n",
    "print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "yaml"
    }
   },
   "outputs": [],
   "source": [
    "# Ollama Configuration File\n",
    "\n",
    "api_version: v1\n",
    "name: physics-rag-chatbot\n",
    "description: A RAG chatbot for answering physics questions using LLaMA3 and LangFlow\n",
    "models:\n",
    "  - name: llama3\n",
    "    version: latest\n",
    "endpoints:\n",
    "  - path: /ask\n",
    "    method: POST\n",
    "    handler: rag_chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# deploy ollama\n",
    "\n",
    "ollama deploy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create knowledge graph using 'rdflib'\n",
    "\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace\n",
    "from rdflib.namespace import FOAF, DC\n",
    "\n",
    "# Create an RDF graph\n",
    "g = Graph()\n",
    "n = Namespace(\"http://example.org/\")\n",
    "\n",
    "# Add data to the graph\n",
    "for doc in dataset:\n",
    "    doc_uri = URIRef(f\"http://example.org/document/{doc['id']}\")\n",
    "    g.add((doc_uri, DC.title, Literal(doc[\"title\"])))\n",
    "    g.add((doc_uri, DC.description, Literal(doc[\"text\"])))\n",
    "    for author in doc.get(\"authors\", []):\n",
    "        g.add((doc_uri, DC.creator, Literal(author)))\n",
    "\n",
    "# Save the graph\n",
    "g.serialize(destination=\"knowledge_graph.rdf\", format=\"xml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Data Visualization in R\n",
    "\n",
    "install.packages(\"ggplot2\")\n",
    "install.packages(\"jsonlite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Load and Visualize Data:\n",
    "\n",
    "library(ggplot2)\n",
    "library(jsonlite)\n",
    "\n",
    "# Load dataset\n",
    "dataset <- fromJSON(\"path/to/your/dataset.json\")\n",
    "\n",
    "# Convert to data frame\n",
    "df <- data.frame(\n",
    "  Title = sapply(dataset, function(x) x$title),\n",
    "  Date = as.Date(sapply(dataset, function(x) x$date))\n",
    ")\n",
    "\n",
    "# Plot the number of documents over time\n",
    "ggplot(df, aes(x = Date)) +\n",
    "  geom_histogram(binwidth = 30) +\n",
    "  labs(title = \"Number of Documents Over Time\",\n",
    "       x = \"Date\",\n",
    "       y = \"Number of Documents\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visuslize.r code explanation\n",
    "\n",
    "Explanation\n",
    "Load the Dataset:\n",
    "\n",
    "The fromJSON function from the jsonlite package is used to load the dataset from a JSON file.\n",
    "Convert to Data Frame:\n",
    "\n",
    "The dataset is converted into a data frame with columns for Title, Date, and TextLength.\n",
    "Plot 1: Number of Documents Over Time:\n",
    "\n",
    "This plot shows the number of documents over time using a histogram. The bin width is set to 30 days.\n",
    "Plot 2: Distribution of Text Lengths:\n",
    "\n",
    "This plot shows the distribution of text lengths (number of characters) of the documents using a histogram.\n",
    "Plot 3: Documents Over Time with Text Length:\n",
    "\n",
    "This plot shows the text length of documents over time using a scatter plot.\n",
    "Save the Plots:\n",
    "\n",
    "The ggsave function is used to save each plot as a PNG file.\n",
    "Running the Script\n",
    "Save the above script as visualize.R and run it using R:\n",
    "\n",
    "bash\n",
    "Copy code\n",
    "Rscript visualize.R\n",
    "This script will generate and save three plots in the current directory:\n",
    "\n",
    "number_of_documents_over_time.png\n",
    "distribution_of_text_lengths.png\n",
    "documents_over_time_with_text_length.png\n",
    "Make sure to update the dataset_path variable with the correct path to your custom dataset JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n-grams using word2vec\n",
    "\n",
    "import os\n",
    "import json\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "# Load the dataset\n",
    "dataset_path = os.getenv(\"DATASET_PATH\", \"path/to/your/custom_dataset.json\")\n",
    "with open(dataset_path, 'r') as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "# Preprocess the dataset: Tokenize and clean text\n",
    "def preprocess_text(text):\n",
    "    return simple_preprocess(text, deacc=True)  # Tokenizes text and removes punctuation\n",
    "\n",
    "texts = [preprocess_text(doc[\"text\"]) for doc in dataset]\n",
    "\n",
    "# Create bigrams and trigrams\n",
    "bigram = Phrases(texts, min_count=5, threshold=100)\n",
    "trigram = Phrases(bigram[texts], threshold=100)\n",
    "\n",
    "bigram_mod = Phraser(bigram)\n",
    "trigram_mod = Phraser(trigram)\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "data_bigrams = make_bigrams(texts)\n",
    "data_trigrams = make_trigrams(texts)\n",
    "\n",
    "# Train a Word2Vec model on the trigram data\n",
    "model = Word2Vec(sentences=data_trigrams, vector_size=100, window=5, min_count=5, workers=4)\n",
    "\n",
    "# Save the model\n",
    "model_path = os.getenv(\"MODEL_PATH\", \"models/word2vec_trigrams.model\")\n",
    "model.save(model_path)\n",
    "\n",
    "# Example: Get embedding for a specific trigram\n",
    "example_trigram = trigram_mod[bigram_mod[preprocess_text(\"This is an example sentence for n-grams\")]]\n",
    "embedding = model.wv[example_trigram[0]]\n",
    "print(f\"Trigram: {example_trigram[0]}, Embedding: {embedding}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation\n",
    "Loading the Dataset:\n",
    "\n",
    "The dataset is loaded from a JSON file specified by the DATASET_PATH environment variable.\n",
    "Preprocessing the Text:\n",
    "\n",
    "The preprocess_text function tokenizes and cleans the text, removing punctuation.\n",
    "Creating Bigrams and Trigrams:\n",
    "\n",
    "Bigrams and trigrams are created using the gensim.models.phrases.Phrases and Phraser classes.\n",
    "make_bigrams and make_trigrams functions are used to generate bigrams and trigrams for the entire dataset.\n",
    "Training the Word2Vec Model:\n",
    "\n",
    "The Word2Vec model is trained on the trigram data.\n",
    "The vector_size parameter specifies the dimensionality of the word vectors.\n",
    "The window parameter specifies the maximum distance between the current and predicted word within a sentence.\n",
    "The min_count parameter ignores all words with total frequency lower than this.\n",
    "Saving the Model:\n",
    "\n",
    "The trained Word2Vec model is saved to the path specified by the MODEL_PATH environment variable.\n",
    "Generating Embeddings:\n",
    "\n",
    "An example sentence is preprocessed into trigrams, and the embedding for the first trigram is printed.\n",
    "\n",
    "Running the Script\n",
    "Save the script as word2vec_ngrams.py and run it:\n",
    "\n",
    "\n",
    "python word2vec_ngrams.py\n",
    "\n",
    ".env file:\n",
    "\n",
    "DATASET_PATH=/home/cmejo/arxiv-dataset/custom_dataset.json\n",
    "MODEL_PATH=models/word2vec_trigrams.model\n",
    "This script will preprocess your text data into bigrams and trigrams, train a Word2Vec model, and save the model for later use. You can then use this model to generate embeddings for any n-gram in your dataset."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
