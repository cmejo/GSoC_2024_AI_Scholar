{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# commands run:\n",
    "\n",
    "## for langchain & langflow environment\n",
    "1. pip install langchain\n",
    "2. pip3 install torch torchvision\n",
    "3. pip install langchain langsmith langchain-community langgraph langchain-cli langchainhub langchain-openai langchain-chroma bs4\n",
    "\n",
    "## langflow start\n",
    "\n",
    "python3 -m langflow run\n",
    "\n",
    "\n",
    "## remote login\n",
    "\n",
    "ssh -C -p 222 -L 7860:127.0.0.1:7860 cmejo@rstudio-tr.braverock.com # langflow http://localhost:7860/ \n",
    "ssh -C -p 222 -L 11434:127.0.0.1:11434 cmejo@rstudio-tr.braverock.com # for ollama http://localhost:11434/ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and preprocess custom dataset\n",
    "\n",
    "import json\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "\n",
    "# Load a pre-trained LLaMA model and tokenizer\n",
    "model_name = \"meta-llama/llama3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Load custom dataset\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "dataset_path = \"path/to/your/dataset.json\"\n",
    "dataset = load_dataset(dataset_path)\n",
    "\n",
    "# Generate embeddings for the dataset\n",
    "def generate_embeddings(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "embeddings_list = [generate_embeddings(doc[\"text\"], tokenizer, model) for doc in dataset]\n",
    "embeddings_array = np.array(embeddings_list)\n",
    "\n",
    "# Create a FAISS index\n",
    "dimension = embeddings_array.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings_array)\n",
    "\n",
    "# Save the index\n",
    "faiss.write_index(index, \"faiss_index.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# langfllow configuration \n",
    "\n",
    "from langchain.chains import SimpleChain\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import Prompt\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Load the FAISS index\n",
    "index = faiss.read_index(\"faiss_index.bin\")\n",
    "\n",
    "# Define a prompt template\n",
    "prompt_template = \"\"\"\n",
    "You are an AI assistant specialized in answering questions based on the provided dataset. Given the following document text, provide a brief and accurate answer to the question:\n",
    "Document: {document}\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Define the function to find the most relevant document\n",
    "def find_relevant_document(question, index, dataset, embeddings):\n",
    "    query_embedding = embeddings.generate_embeddings(question).reshape(1, -1)\n",
    "    D, I = index.search(query_embedding, k=1)\n",
    "    document = dataset[I[0][0]][\"text\"]\n",
    "    return document\n",
    "\n",
    "# Define the chatbot function\n",
    "def rag_chatbot(question, index, dataset, embeddings):\n",
    "    document = find_relevant_document(question, index, dataset, embeddings)\n",
    "    prompt = Prompt(prompt_template.format(document=document, question=question))\n",
    "    answer = prompt.run()\n",
    "    return answer\n",
    "\n",
    "# Example usage\n",
    "question = \"What is quantum entanglement?\"\n",
    "answer = rag_chatbot(question, index, dataset, HuggingFaceEmbeddings(model, tokenizer))\n",
    "print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "yaml"
    }
   },
   "outputs": [],
   "source": [
    "# Ollama Configuration File\n",
    "\n",
    "api_version: v1\n",
    "name: physics-rag-chatbot\n",
    "description: A RAG chatbot for answering physics questions using LLaMA3 and LangFlow\n",
    "models:\n",
    "  - name: llama3\n",
    "    version: latest\n",
    "endpoints:\n",
    "  - path: /ask\n",
    "    method: POST\n",
    "    handler: rag_chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# deploy ollama\n",
    "\n",
    "ollama deploy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create knowledge graph using 'rdflib'\n",
    "\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace\n",
    "from rdflib.namespace import FOAF, DC\n",
    "\n",
    "# Create an RDF graph\n",
    "g = Graph()\n",
    "n = Namespace(\"http://example.org/\")\n",
    "\n",
    "# Add data to the graph\n",
    "for doc in dataset:\n",
    "    doc_uri = URIRef(f\"http://example.org/document/{doc['id']}\")\n",
    "    g.add((doc_uri, DC.title, Literal(doc[\"title\"])))\n",
    "    g.add((doc_uri, DC.description, Literal(doc[\"text\"])))\n",
    "    for author in doc.get(\"authors\", []):\n",
    "        g.add((doc_uri, DC.creator, Literal(author)))\n",
    "\n",
    "# Save the graph\n",
    "g.serialize(destination=\"knowledge_graph.rdf\", format=\"xml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Data Visualization in R\n",
    "\n",
    "install.packages(\"ggplot2\")\n",
    "install.packages(\"jsonlite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Load and Visualize Data:\n",
    "\n",
    "library(ggplot2)\n",
    "library(jsonlite)\n",
    "\n",
    "# Load dataset\n",
    "dataset <- fromJSON(\"path/to/your/dataset.json\")\n",
    "\n",
    "# Convert to data frame\n",
    "df <- data.frame(\n",
    "  Title = sapply(dataset, function(x) x$title),\n",
    "  Date = as.Date(sapply(dataset, function(x) x$date))\n",
    ")\n",
    "\n",
    "# Plot the number of documents over time\n",
    "ggplot(df, aes(x = Date)) +\n",
    "  geom_histogram(binwidth = 30) +\n",
    "  labs(title = \"Number of Documents Over Time\",\n",
    "       x = \"Date\",\n",
    "       y = \"Number of Documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
